head     1.1;
branch   1.1.1;
access   ;
symbols  r1:1.1.1.1 mhelal:1.1.1;
locks    ; strict;
comment  @# @;


1.1
date     2008.05.06.04.03.23;  author mhelal;  state Exp;
branches 1.1.1.1;
next     ;

1.1.1.1
date     2008.05.06.04.03.23;  author mhelal;  state Exp;
branches ;
next     ;


desc
@@



1.1
log
@Initial revision
@
text
@\contentsline {chapter}{\hbox to\@@tempdima {\hfil }Abstract}{iii}
\contentsline {chapter}{\hbox to\@@tempdima {\hfil }Declaration}{v}
\contentsline {chapter}{\hbox to\@@tempdima {\hfil }Acknowledgements}{vi}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Dimensionality Curse in High Dimensional Scientific Problems}{1}
\contentsline {section}{\numberline {1.2}Moore's Law Continuty through Multi-Cores, HPC, and Grid Computing}{3}
\contentsline {section}{\numberline {1.3}Thesis Motivations, Aims and Objectives}{3}
\contentsline {section}{\numberline {1.4}Thesis Scope}{3}
\contentsline {section}{\numberline {1.5}Contributions and Impact}{4}
\contentsline {section}{\numberline {1.6}Research Method}{4}
\contentsline {section}{\numberline {1.7}Thesis Organization}{5}
\contentsline {chapter}{\numberline {2}Literature Review and Problem Definition}{6}
\contentsline {section}{\numberline {2.1}Array Functions & Operators History}{6}
\contentsline {subsection}{\numberline {2.1.1}Conformal Computing Methods}{8}
\contentsline {subsection}{\numberline {2.1.2}Mathematics of Arrays}{9}
\contentsline {subsection}{\numberline {2.1.3}Mathematical Background}{11}
\contentsline {section}{\numberline {2.2}Distributed Procession - Capacity \& Capability Computing}{12}
\contentsline {subsection}{\numberline {2.2.1}Capacity Computing}{12}
\contentsline {subsection}{\numberline {2.2.2}Capability Computing}{12}
\contentsline {subsection}{\numberline {2.2.3}High Performance Machines}{12}
\contentsline {subsection}{\numberline {2.2.4}Parallel Processing Models}{12}
\contentsline {section}{\numberline {2.3}Dynamic Programming}{12}
\contentsline {section}{\numberline {2.4}Dimensionality Reduction Methods and Optimality Effects}{12}
\contentsline {section}{\numberline {2.5}Multiple Sequence Alignment in Computational Biology}{12}
\contentsline {subsection}{\numberline {2.5.1}Global Alignment}{14}
\contentsline {subsection}{\numberline {2.5.2}Local Alignment}{17}
\contentsline {subsection}{\numberline {2.5.3}Benchmark Alignment databases}{27}
\contentsline {subsection}{\numberline {2.5.4}Multiple Sequence Alignment Methods}{29}
\contentsline {subsubsection}{Hidden Markov model}{29}
\contentsline {subsubsection}{Progressive Alignment Techniques}{30}
\contentsline {subsubsection}{Iterative Alignment techniques}{33}
\contentsline {subsubsection}{Simultaneous Alignment}{35}
\contentsline {subsection}{\numberline {2.5.5}Existing Methods Problems}{36}
\contentsline {chapter}{\numberline {3}Proposed Initial Master/Slave Solution}{37}
\contentsline {section}{\numberline {3.1}Sequential MSA using MOA}{37}
\contentsline {subsection}{\numberline {3.1.1}Invariance of Shape and Dimension}{37}
\contentsline {subsection}{\numberline {3.1.2}Mapping Operations to Index Transformations}{37}
\contentsline {section}{\numberline {3.2}Distributed Master/Slave MSA using MoA}{38}
\contentsline {section}{\numberline {3.3}Partitioning MSA using MoA}{38}
\contentsline {subsection}{\numberline {3.3.1}Analysis of Dependency}{39}
\contentsline {subsection}{\numberline {3.3.2}Partitioning Scheme}{41}
\contentsline {subsection}{\numberline {3.3.3}Distributed Scoring Requirements}{45}
\contentsline {subsection}{\numberline {3.3.4}Distributed Trace Back Requirements}{47}
\contentsline {subsection}{\numberline {3.3.5}Scalability Issues}{47}
\contentsline {subsection}{\numberline {3.3.6}Portability Issues}{50}
\contentsline {chapter}{\numberline {4}Peer to Peer Solution}{51}
\contentsline {section}{\numberline {4.1}P2P Partitioning}{51}
\contentsline {section}{\numberline {4.2}Wave Calculations}{52}
\contentsline {section}{\numberline {4.3}Processor Assignments and load balancing}{61}
\contentsline {section}{\numberline {4.4}P2P Scoring and Dependency Communication}{67}
\contentsline {section}{\numberline {4.5}Complexity Analysis}{68}
\contentsline {section}{\numberline {4.6}Implementation Details and Optimization}{70}
\contentsline {subsection}{\numberline {4.6.1}Checkpointing and Restoration}{70}
\contentsline {subsection}{\numberline {4.6.2}Optimization}{70}
\contentsline {chapter}{\numberline {5}Linear Optimization}{74}
\contentsline {section}{\numberline {5.1}Multi-dimensional Optimization}{74}
\contentsline {chapter}{\numberline {6}Reduced Search Space and Classification}{75}
\contentsline {chapter}{\numberline {7}Partitioning, Scheduling, and Communication Aspects}{76}
\contentsline {section}{\numberline {7.1}Partitioning Aspects}{76}
\contentsline {subsection}{\numberline {7.1.1}Partition Size Effects}{76}
\contentsline {subsection}{\numberline {7.1.2}Reshaping the Tensor to retrieve partitions}{77}
\contentsline {section}{\numberline {7.2}Scheduling Methods}{78}
\contentsline {subsection}{\numberline {7.2.1}Master/Slave Scheduling}{80}
\contentsline {subsubsection}{Bag of Tasks}{80}
\contentsline {subsubsection}{Round Robin}{80}
\contentsline {subsubsection}{MOA Scheduling}{80}
\contentsline {subsection}{\numberline {7.2.2}Peer-to-Peer Scheduling}{80}
\contentsline {section}{\numberline {7.3}Load Balancing}{80}
\contentsline {subsection}{\numberline {7.3.1}Mathematical Proof of Optimality of Load Balancing}{81}
\contentsline {section}{\numberline {7.4}Concurrency \& Communication Aspects }{81}
\contentsline {subsection}{\numberline {7.4.1}Threads Conditions Vs Semaphores}{81}
\contentsline {subsection}{\numberline {7.4.2}Blocking Vs. Non-Blocking Communication}{81}
\contentsline {subsection}{\numberline {7.4.3}Slave / Slave Coupling Issues}{81}
\contentsline {section}{\numberline {7.5}MoA as Communication Modelling Language}{81}
\contentsline {chapter}{\numberline {8}Results Analysis}{82}
\contentsline {section}{\numberline {8.1}Execution Results}{82}
\contentsline {subsection}{\numberline {8.1.1}Full Search Space Execution Results}{82}
\contentsline {subsubsection}{Sequential Execution Results}{82}
\contentsline {subsubsection}{Distributed Master/Slave Execution Results}{82}
\contentsline {subsubsection}{Distributed Peer-to-Peer Execution Results}{83}
\contentsline {subsection}{\numberline {8.1.2}Reduced Search Space Execution Results}{92}
\contentsline {subsection}{\numberline {8.1.3}Classification Execution Results}{92}
\contentsline {section}{\numberline {8.2}Partition Size Scalability}{92}
\contentsline {section}{\numberline {8.3}Processors Scalability}{92}
\contentsline {section}{\numberline {8.4}Comparison with Existing Methods}{92}
\contentsline {section}{\numberline {8.5}Re-Configurable Execution Parameters}{92}
\contentsline {chapter}{\numberline {9}Conclusion and Future Work}{93}
\contentsline {section}{\numberline {9.1}Thesis Contribution and Conclusion}{93}
\contentsline {subsection}{\numberline {9.1.1}Optimal and Near Optimal Distributed MSA}{95}
\contentsline {subsection}{\numberline {9.1.2}Generic High Dimensional Partitioning Method}{95}
\contentsline {subsection}{\numberline {9.1.3}Dependency Modelling and Reduction Technique}{95}
\contentsline {subsection}{\numberline {9.1.4}Generic Scheduler with load balancing}{95}
\contentsline {section}{\numberline {9.2}Future Work}{95}
\contentsline {subsection}{\numberline {9.2.1}Scores Caching Through Multi-Level Linked List Data Structure}{95}
\contentsline {subsection}{\numberline {9.2.2}Fixed vs Variable Partitioning, and Suboptimal Solutions Retrieval}{95}
\contentsline {subsection}{\numberline {9.2.3}Optimization and Heuristics}{95}
\contentsline {subsection}{\numberline {9.2.4}Employing Dimensionality Reduction Techniques}{95}
\contentsline {subsection}{\numberline {9.2.5}Further Problems to benefit from proposed Methods}{95}
@


1.1.1.1
log
@Thesis Writing
@
text
@@
